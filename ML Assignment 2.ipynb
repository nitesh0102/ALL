{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba474c3-430e-40c9-9071-f4d495fbcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ff7d2-669a-4e31-86aa-1aa508a03899",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias : When the model is not able to comprehend the complex relations between input and output and make overly simplistic assumptions. \n",
    "These assumptions make the learning easy but does not capture the underlying complexities of the data. \n",
    "when a model has poor performance both on training and testing data means high bias because of the simple model, Indicating UNDERFITTING. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355e73c-1211-431a-9b0e-6046f94e5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variance : High variance occurs When the model learns noise and random fluctuations instead of the underlying pattern in the training data\n",
    "and performs well on the training data and poorly on the test data, indicating overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f1f69-b270-45e6-a935-68810e0b8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "High Bias (Overly Simple Model): Underfitting \n",
    "Low Bias & Low Variance : GoodFitting\n",
    "High Variance : Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837ff8e-4f1c-4808-b0eb-2635ab7e88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities.\n",
    "It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data.\n",
    "In simple terms, an underfit modelâ€™s are inaccurate, especially when applied to new, unseen examples. \n",
    "It mainly happens when we uses very simple model with overly simplified assumptions. To address underfitting problem of the model, \n",
    "we need to use more complex models, with enhanced feature representation, and less regularization.\n",
    "\n",
    "Note: The underfitting model has High bias and low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a5629-dd3d-4b60-8415-1454bfe7887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A statistical model is called overfitted when it does not make accuarte predictions on test data.\n",
    "    When a model gets trained on so much data, it starts learning from noise and innacurate data entries in our dataset, and when \n",
    "testing with the test data results in high variance. Then the model does not categorize the data correctly beacuse of too much noise. \n",
    "    The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms \n",
    "have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. \n",
    "   A solution to avoid overfitting is using a linear algorithm if we have linear data \n",
    "or using the parameters like the maximal depth if we are using decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccffcdd-eb46-4a2e-b96b-ce9f7291aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a nutshell, Overfitting is a problem where the evaluation of machine learning algorithms on training data is different from unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c33504-f20a-41ef-a384-55d319f8a5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a008bc-b639-44a8-9623-2529bf4c9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9ec83-ed3c-4842-8d51-a7ce8cee2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "Overfitting is an undesirable machine learning behaviour where the machine learning model gives good result on the training data \n",
    "but behaves poorly on unseen (test) data.\n",
    "\n",
    "    Overfitting occurs when the model does not generalize and fits to closely to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07227c9f-d100-4c81-b887-0978b35e1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reasons for overfitting to happen: \n",
    "1. When the data size is too small hence the model is not able to understand all possible input data values.\n",
    "2. when the training data contains too much noise and irrelevant information\n",
    "3. The model trains too long on a simple training data\n",
    "4. The  model complexity is high so that it learns noise within the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a967f77-8d26-43c9-b9be-75440d9d07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Overfitting examples\n",
    "Consider a use case where a machine learning model has to analyze photos and identify the ones that contain dogs in them. If the machine learning model was trained on a data set that contained majority photos showing dogs outside in parks ,\n",
    "it may may learn to use grass as a feature for classification, and may not recognize a dog inside a room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ed6e1-dd89-431b-8ed2-e857ad4602d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Another overfitting example is a machine learning algorithm that predicts a university student's academic performance and graduation outcome by analyzing several factors like family income, past academic performance, and academic qualifications of parents. However, the test data only includes candidates from a specific gender or ethnic group. \n",
    "In this case, overfitting causes the algorithm's prediction accuracy to drop for candidates with gender or ethnicity outside of the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb81b4-4773-4293-8bd6-be3c56a0e7ef",
   "metadata": {},
   "source": [
    "## How to detect overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218d406-0816-4264-b6e9-fb3b1925d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "The best way to detect overfitting is by testing the model on more complex data. Typically part of the training data is used for \n",
    "testing the model for overfitting. A high error rate in testing data indicates overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf30f0d-ac5a-48ca-a1a1-5be673966902",
   "metadata": {},
   "outputs": [],
   "source": [
    "One method to use for testing of overfitting is K fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e834c1-cb56-4b90-9301-7765f76d5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this method data is divide in K equal size of subsets called fold. The training process consists of a series of iterations, during \n",
    "each iterations the steps are: \n",
    "1. Keep one subset as Validation data and train the machine learning model on the remaining K-1 subsets. \n",
    "2. Observe how the model performns on the validation sample. \n",
    "3. Score model performance based on output data quality. \n",
    "\n",
    "Iterations repeat until you test the model on every sample set. \n",
    "You then average the scores across all iterations to get the final assessment of the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52172861-49cf-4bb5-90e5-8ad2893582a3",
   "metadata": {},
   "source": [
    "## How can you prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584930d-1190-4e73-95d1-a54af4459c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "You can prevent overfitting by diversifying and scaling your training data set or using some other data science strategies, like those given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ee51b-eeec-4693-84eb-3080f76ef0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Early Stopping : Pausing the training before the model learns the noise in the data.\n",
    "However getting the timing right is very important else the model will still not give accuarate results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d9e76-a422-4c6b-ba24-a5d196f36dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pruning : Pruning is feature selection which means identifying the features which are most important and eliminating the not important ones. \n",
    "\n",
    "For example, to predict if an image is an animal or human, you can look at various input parameters like face shape, ear position, body structure, etc.\n",
    "You may prioritize face shape and ignore the shape of the eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da273c7e-5f25-4f5a-a509-a11a2aee8bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization : Regularization is collection of training/optimization techinque used to reduce overfitting.\n",
    "    These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance. \n",
    "For example, mathematical calculations apply a penalty value to features with minimal impact. \n",
    "Consider a statistical model attempting to predict the housing prices of a city in 20 years. Regularization would give a lower penalty value to features like population growth and average annual income \n",
    "but a higher penalty value to the average annual temperature of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568a33f-c932-40fc-893f-647a5fcb7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensembling : Ensemble combines results/predcitions from several machine learning models. some models are called weak learners \n",
    "because their prediction are poor. Ensemble method combine all the weak learners to get more accurate results. They use multiple models\n",
    "to analyze sample data and pick the most accurate outcomes. \n",
    "\n",
    "The tow main ensemble methods are bagging and boosting. \n",
    "Boosting trains model one after another to get the final result, while bagging trains them in parallel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d6d6e-11b0-4895-8b28-440f4f7c5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Augmentation : Data augmentation is machine learning technique that changes the sample data silghtly every time the model processes it. \n",
    "\n",
    "When done in moderation the traning data appears unique to the model and prevent the model from learning their characteristics. F\n",
    "\n",
    "For example applying transformations such as translation, flipping and rotation to the input images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0188932-5a9c-4f2c-868a-b4c07ae2ea09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21e6fd5-9cda-4e94-91dc-6ba7293d5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e42e50-aac8-4062-bdbb-09688dc8a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Underfitting occurs when the model does not able to understand the complexities and underlying pattern of the data, this mostly\n",
    "happens when we use overly simplistic models or the training time is short. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcaa58f-c0aa-46f9-bfd7-c44c8caafc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "High Bias : Underfitting\n",
    "High Variance : Overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807231c-920d-4b57-a949-35aea7de07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "More model training results in less bias but variance can increase. Data scientists aim to find the sweet spot between underfitting and overfitting when fitting a model.\n",
    "A well-fitted model can quickly establish the dominant trend for seen and unseen data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e40c2-53fd-45e5-b2d2-2d055a7927db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09a201d-28ad-4b13-b5e5-83c1131e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32a31b-0d82-4528-b653-460f8e0217f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias Variance Tradeoff\n",
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone.\n",
    "If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. \n",
    "In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off. \n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm canâ€™t be more complex and less complex at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356b462-ca8c-4b24-b2c9-24fc4f4dbc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    " We try to optimize the value of the total error for the model by using the Bias-Variance Tradeoff.\n",
    "\n",
    "TotalError = Bias^2 + Variance + Irreducible Error\n",
    "\n",
    "The best fit will be given by the hypothesis on the tradeoff point.  \n",
    "This is referred to as the best point chosen for the training of the algorithm which gives low error in training as well as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af784d26-945d-4a9e-9068-65dd2659d85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39767b3e-355d-441e-bb40-bb2db4e098ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ea939-2003-476e-b8b2-aab7d5887f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning curve can be used to detrmine if the model is best fit, underfit or overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa991c3-0f4f-42ee-aa95-2e18bc95256d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af069fb-48da-4c2f-83c1-29a671cd99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc0450-5139-420e-aec4-968800cabad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
